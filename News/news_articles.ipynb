{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a9e5a3",
   "metadata": {},
   "source": [
    "## News Sentiment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3f1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data & numerical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime  # provides date and time manipulation functions\n",
    "\n",
    "# Core / stdlib\n",
    "import os\n",
    "import time\n",
    "os.listdir()\n",
    "\n",
    "# Networking / HTTP\n",
    "import requests   # fetch pages when Selenium fails\n",
    "\n",
    "# HTML parsing & article extraction\n",
    "from bs4 import BeautifulSoup  # parse page DOM\n",
    "from newspaper import Article  # fallback article text extraction\n",
    "\n",
    "# NLP / tokenization & models\n",
    "from nltk import sent_tokenize  # sentence splitter \n",
    "from transformers import pipeline  # Hugging Face inference pipeline\n",
    "\n",
    "# Browser automation\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # auto-download ChromeDriver\n",
    "\n",
    "# Visualization / plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c206e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Code_Louisville\\Course_Work\\News\\News_Sentiment\n"
     ]
    }
   ],
   "source": [
    "# Configure headless Chrome for scraping (safer in CI)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')                 # run without opening a browser window\n",
    "options.add_argument('--no-sandbox')               # required in some restricted environments\n",
    "options.add_argument('--disable-dev-shm-usage')    # avoid /dev/shm issues in containers\n",
    "\n",
    "# Start ChromeDriver (auto-downloads compatible driver)\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "# Create project folder and switch to it so all outputs are saved there\n",
    "PROJECT_DIR = \"News_Sentiment\"\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(os.getcwd())\n",
    "RUN_DATE = datetime.now().strftime(\"%Y-%m-%d\")  # stamp each manual run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write text to Readme.md\n",
    "readme_text = \"\"\"# News Sentiment Pipeline\n",
    "\n",
    "### This code collected seven days' worth of unstructured news-article data and separated it into daily output files for future analysis.\n",
    "\n",
    "### ***Code Outline***\n",
    "* Create News_Sentiment folder, prepare README.md, set working directory, record run date\n",
    "* Configure headless ChromeDriver (webdriver-manager) and start Selenium\n",
    "* Load BBC News, collect deduplicated /news/ links (limit applied)\n",
    "* For each URL: fetch HTML via Selenium with requests fallback\n",
    "* Extract article text (BeautifulSoup selectors, newspaper3k fallback), clean and filter short items\n",
    "* Build DataFrame with title, url, text, run_date, and text length\n",
    "* Load NLTK sentence tokenizer and HF sentiment pipeline (DistilBERT on CPU)\n",
    "* Split articles into sentences, run batched sentence-level sentiment, convert to signed scores\n",
    "* Expand sentence results to a long table and save sentences_all_articles.csv\n",
    "* Save per-article CSVs and top-sentences text files under ./sentences/\n",
    "* Aggregate per-article polarity counts and mean score, save articles_sentiment_summary.csv and articles_sentiment.csv\n",
    "* Write human-readable articles_texts.txt archive\n",
    "* Log fetch/parse/sentiment errors, suppress HF symlink warning if set, and quit Selenium driver\n",
    "* File aggregation and histogram visualization\n",
    "\n",
    "\n",
    "### ***Link***\n",
    "[BBC News Articles](https://www.bbc.com/news) \n",
    "\n",
    "\n",
    "### ***Directory***\n",
    "| Column Name         | Description                                                                          | Data Type |\n",
    "|:--------------------|:-------------------------------------------------------------------------------------|:---------:|\n",
    "| article_index       | Unique identifier for each article                                                   | Integer   |\n",
    "| title               | Title of the article                                                                 | String    |\n",
    "| url                 | URL link to the article                                                              | String    |\n",
    "| avg_label           | Average sentiment label (e.g., POSITIVE, NEGATIVE)                                   | String    |\n",
    "| avg_score           | Average absolute signed score (0-1)                                                  | Float     |\n",
    "| num_sentences       | Total number of sentences in the article                                             | Integer   |\n",
    "| sentence            | A specific sentence extracted from the article                                       | String    |\n",
    "| sent_label          | Sentiment label for the specific sentence (e.g., POSITIVE, NEGATIVE)                 | String    |\n",
    "| sent_score          | Confidence score (0–1)                                                               | Float     |\n",
    "| sent_value          | A numerical representation of the sentence score, possibly reflecting its sentiment  | Float     |\n",
    "\n",
    "\n",
    "### ***Project Setup & Run Instructions***\n",
    "|Action\t                                                |Command / Notes                                                                                           |\n",
    "|:------------------------------------------------------|:---------------------------------------------------------------------------------------------------------|\n",
    "|Clone or fork the repo                                 |git clone https://github.com/ragogzheyan/Course_Work/tree/main/News                                       |\n",
    "|Create & activate virtualenv (Git Bash)                |python -m venv venv         source venv/Scripts/activate                                                  | \n",
    "|Upgrade pip                                            |python -m pip install --upgrade pip                                                                       |\n",
    "|Install from requirements.txt\t                        |pip install -r requirements.txt                                                                           |\n",
    "|Install packages manually (if no requirements.txt)  \t|pip install selenium webdriver-manager beautifulsoup4 newspaper3k nltk transformers pandas numpy requests | \n",
    "|NLTK tokenizer setup (one-time)\t                    |python -c \"import nltk; nltk.download('punkt')\"                                                           |\n",
    "|Run the pipeline (Jupyter notebook)\t                |jupyter lab or jupyter notebook — then open and run the project's notebook (e.g., run_pipeline.ipynb)     |\n",
    "|Deactivate virtualenv (Git Bash)\t                    |deactivate                                                                                                |\n",
    "\n",
    "\n",
    "### ***AI tools & Provenance(APA)***\n",
    "**Model**: GPT-5((OpenAI; accessed 2025-11-07-2025-11-14) \n",
    "**Role**: Assisted with Selenium configuration, rule generation, and sentence-level sentiment processing. \n",
    "**Prompts & outputs**: Archived: not available — prompts and important AI outputs are summarized below. \n",
    "**Settings**: model=gpt-5 \n",
    "\n",
    "#### ***Selenium Fetch & Extraction***  \n",
    "**Selenium config**: headless ChromeDriver via webdriver-manager; short wait (time.sleep(1)) after driver.get() to allow JS rendering. \n",
    "**Fetch logic**: Try Selenium first; if page_source length < 500 or Selenium errors, fallback to requests with timeout=8. \n",
    "**Extraction logic**: BeautifulSoup selectors: article, [role='main'], .story-body, .ssrcss-uf6wea-RichTextComponentWrapper; accept only extracts >200 chars — otherwise fallback to newspaper3k parsing. \n",
    "**Failure behavior**: On fetch/parse exceptions return empty string; errors. \n",
    "\"\"\"\n",
    "with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a586e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BBC News homepage and wait briefly for dynamic content\n",
    "url = \"https://www.bbc.com/news\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# Parse the rendered page with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Collect article links under /news/, ignore image links, deduplicate, and keep top 18\n",
    "links = []\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    href = a['href']\n",
    "    if href.startswith(\"/news/\") and not href.lower().endswith((\".png\", \".jpg\", \".gif\")):\n",
    "        full = \"https://www.bbc.com\" + href if href.startswith(\"/\") else href\n",
    "        links.append(full)\n",
    "links = list(dict.fromkeys(links))[:18]  # dedupe while preserving order, then limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48305287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article_html(url):\n",
    "    # Try Selenium first (handles JS-rendered pages)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1)                     # short wait for dynamic content\n",
    "        html = driver.page_source\n",
    "        if len(html) < 500:               # heuristic: very short HTML likely incomplete\n",
    "            raise ValueError(\"Short html from Selenium\")\n",
    "        return html\n",
    "    except Exception:\n",
    "        # Fallback to requests for static pages or if Selenium fails\n",
    "        try:\n",
    "            r = requests.get(url, timeout=8)\n",
    "            r.raise_for_status()\n",
    "            return r.text\n",
    "        except Exception as e:\n",
    "            print(\"Fetch failed:\", url, e)\n",
    "            return \"\"                      # return empty string on complete failure\n",
    "\n",
    "def extract_text_from_html(html, url=\"\"):\n",
    "    # Extract main article text using common selectors first, then newspaper3k fallback\n",
    "    try:\n",
    "        s = BeautifulSoup(html, \"html.parser\")\n",
    "        # Common containers: <article>, role=main, legacy story-body, BBC-specific class\n",
    "        parts = s.select(\"article, [role='main'], .story-body, .ssrcss-uf6wea-RichTextComponentWrapper\")\n",
    "        if parts:\n",
    "            # Join visible text while preserving sentence spacing\n",
    "            text = \" \".join(p.get_text(separator=\" \", strip=True) for p in parts)\n",
    "            if len(text) > 200:           # require a minimum length to accept extraction\n",
    "                return text\n",
    "        # Fallback: use newspaper3k to parse article heuristically\n",
    "        a = Article(url)\n",
    "        a.set_html(html)\n",
    "        a.parse()\n",
    "        return a.text or \"\"\n",
    "    except Exception as e:\n",
    "        print(\"Parse failed:\", url, e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ff9898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>run_date</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NewsNews</td>\n",
       "      <td>https://www.bbc.com/news/war-in-ukraine</td>\n",
       "      <td>News News War in Ukraine Fog helps Russian for...</td>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>5212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NewsNews</td>\n",
       "      <td>https://www.bbc.com/news/world/africa</td>\n",
       "      <td>News News Africa Nigeria cancels mother-tongue...</td>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>6923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NewsNews</td>\n",
       "      <td>https://www.bbc.com/news/world/asia</td>\n",
       "      <td>News News Asia China India Indian police inves...</td>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>4727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NewsNews</td>\n",
       "      <td>https://www.bbc.com/news/scotland</td>\n",
       "      <td>News News Scotland Scotland Politics Hitman pl...</td>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>6868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NewsNews</td>\n",
       "      <td>https://www.bbc.com/news/wales</td>\n",
       "      <td>News News Wales Wales Politics Man, 18, arrest...</td>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>6907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title                                      url  \\\n",
       "1   NewsNews  https://www.bbc.com/news/war-in-ukraine   \n",
       "12  NewsNews    https://www.bbc.com/news/world/africa   \n",
       "13  NewsNews      https://www.bbc.com/news/world/asia   \n",
       "8   NewsNews        https://www.bbc.com/news/scotland   \n",
       "10  NewsNews           https://www.bbc.com/news/wales   \n",
       "\n",
       "                                                 text    run_date  text_len  \n",
       "1   News News War in Ukraine Fog helps Russian for...  2025-11-13      5212  \n",
       "12  News News Africa Nigeria cancels mother-tongue...  2025-11-13      6923  \n",
       "13  News News Asia China India Indian police inves...  2025-11-13      4727  \n",
       "8   News News Scotland Scotland Politics Hitman pl...  2025-11-13      6868  \n",
       "10  News News Wales Wales Politics Man, 18, arrest...  2025-11-13      6907  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build dataset: iterate collected links, fetch & extract article text, and store rows\n",
    "rows = []\n",
    "for u in links:\n",
    "    try:\n",
    "        html = fetch_article_html(u)              # fetch page (Selenium then requests)\n",
    "        if not html:\n",
    "            continue                              # skip if fetch failed\n",
    "        text = extract_text_from_html(html, u)    # extract main article text\n",
    "        s = BeautifulSoup(html, \"html.parser\")\n",
    "        # Prefer <h1> for title, fallback to <title>; safe-get with empty default\n",
    "        title = (s.find(\"h1\") or s.title).get_text(strip=True) if (s.find(\"h1\") or s.title) else \"\"\n",
    "        text = text.replace(\"Advertisement\", \"\").strip()  # remove common noise\n",
    "        if len(text) < 120:                       # skip short/insufficient articles\n",
    "            continue\n",
    "        rows.append({\"title\": title, \"url\": u, \"text\": text, \"run_date\": RUN_DATE})\n",
    "    except Exception as e:\n",
    "        print(\"Article loop error:\", u, e)        # log and continue on errors\n",
    "\n",
    "# Convert to DataFrame and enforce minimum text length\n",
    "df = pd.DataFrame(rows).reset_index(drop=True)\n",
    "df['text_len'] = df['text'].str.len()             # helpful for filtering/analysis\n",
    "df = df[df['text_len'] > 120].reset_index(drop=True)\n",
    "df.sample(5)                                        # quick preview in interactive runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e87a35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       title                                                url avg_label  \\\n",
      "0   NewsNews       https://www.bbc.com/news/topics/c2vdnvdg6xxt  NEGATIVE   \n",
      "1   NewsNews            https://www.bbc.com/news/war-in-ukraine  NEGATIVE   \n",
      "2   NewsNews                 https://www.bbc.com/news/us-canada  NEGATIVE   \n",
      "3   NewsNews                        https://www.bbc.com/news/uk  NEGATIVE   \n",
      "4   NewsNews                  https://www.bbc.com/news/politics  NEGATIVE   \n",
      "5   NewsNews                   https://www.bbc.com/news/england  NEGATIVE   \n",
      "6   NewsNews          https://www.bbc.com/news/northern_ireland  NEGATIVE   \n",
      "7   NewsNews  https://www.bbc.com/news/northern_ireland/nort...  NEGATIVE   \n",
      "8   NewsNews                  https://www.bbc.com/news/scotland  NEGATIVE   \n",
      "9   NewsNews  https://www.bbc.com/news/scotland/scotland_pol...  NEGATIVE   \n",
      "10  NewsNews                     https://www.bbc.com/news/wales  NEGATIVE   \n",
      "11  NewsNews      https://www.bbc.com/news/wales/wales_politics  NEGATIVE   \n",
      "12  NewsNews              https://www.bbc.com/news/world/africa  NEGATIVE   \n",
      "13  NewsNews                https://www.bbc.com/news/world/asia  NEGATIVE   \n",
      "14  NewsNews          https://www.bbc.com/news/world/asia/china  NEGATIVE   \n",
      "15  NewsNews          https://www.bbc.com/news/world/asia/india  NEGATIVE   \n",
      "16  NewsNews           https://www.bbc.com/news/world/australia  NEGATIVE   \n",
      "17  NewsNews              https://www.bbc.com/news/world/europe  NEGATIVE   \n",
      "\n",
      "    avg_score  num_sentences                   polarity_counts  \\\n",
      "0    0.828146             27   {'positive': 2, 'negative': 25}   \n",
      "1    0.691494             30   {'positive': 4, 'negative': 26}   \n",
      "2    0.662296             36   {'positive': 6, 'negative': 30}   \n",
      "3    0.667098             35   {'positive': 6, 'negative': 29}   \n",
      "4    0.630501             36   {'positive': 6, 'negative': 30}   \n",
      "5    0.436125             39  {'positive': 11, 'negative': 28}   \n",
      "6    0.245688             45  {'positive': 17, 'negative': 28}   \n",
      "7    0.812646             12   {'positive': 1, 'negative': 11}   \n",
      "8    0.386821             42  {'positive': 13, 'negative': 29}   \n",
      "9    0.530118             19   {'positive': 4, 'negative': 15}   \n",
      "10   0.000796             43  {'positive': 21, 'negative': 22}   \n",
      "11   0.815547             12   {'positive': 1, 'negative': 11}   \n",
      "12   0.240132             42  {'positive': 16, 'negative': 26}   \n",
      "13   0.336729             27   {'positive': 9, 'negative': 18}   \n",
      "14   0.440816             25   {'positive': 7, 'negative': 18}   \n",
      "15   0.359957             26   {'positive': 8, 'negative': 18}   \n",
      "16   0.442424             27   {'positive': 7, 'negative': 20}   \n",
      "17   0.395767             30   {'positive': 9, 'negative': 21}   \n",
      "\n",
      "                                         top_positive  \\\n",
      "0   [Bowen: Trump's role in Gaza ceasefire was dec...   \n",
      "1   [On the ground The 'Heroes of Kharkiv' who sav...   \n",
      "2   [Watch: Trump and Obama honour US troops for V...   \n",
      "3   [Treasure finds in England reach record high -...   \n",
      "4   [Morgan McSweeney is credited with mastermindi...   \n",
      "5   [Prince opens new access road he helped fund a...   \n",
      "6   [The Nolan Show Breaking news and hard-hitting...   \n",
      "7   [Any tax increase must be 'fair and progressiv...   \n",
      "8   [Good Morning Scotland The nation's morning ne...   \n",
      "9   [SNP take over running of Stirling Council An ...   \n",
      "10  [Features & analysis Northern Lights leave sky...   \n",
      "11  [5 hrs ago Wales UK's first small nuclear powe...   \n",
      "12  [Panache, parades and power: Africa's top shot...   \n",
      "13  [12 hrs ago 13 hrs ago Thai king becomes count...   \n",
      "14  [Handshakes and whispers: Trump and Xi's meeti...   \n",
      "15  [Fans celebrate India women's World Cup win In...   \n",
      "16  [Fatal attack revives debate over controversia...   \n",
      "17  [How an Afghan child bride became one of Europ...   \n",
      "\n",
      "                                         top_negative  \n",
      "0   [UN's top court says Israel obliged to allow U...  \n",
      "1   [The new measures target Moscow's ability to f...  \n",
      "2   [The White House says they are an attempt to \"...  \n",
      "3   [2 hrs ago UK Epstein email says Andrew had ph...  \n",
      "4   [As crises pile up, can the Home Office be fix...  \n",
      "5   [Runner's half marathon holiday 'ruined' by fo...  \n",
      "6   [9 hrs ago 10 hrs ago 'Big disappointment' as ...  \n",
      "7   [Guidance for businesses on NI's post-Brexit t...  \n",
      "8   [5 hrs ago Scotland Politics Tesco suspends fi...  \n",
      "9   [Finance secretary may have to 'revisit' no ta...  \n",
      "10  [2 days ago Wales Lorry driver who blamed fata...  \n",
      "11  [1 day ago Wales  Jobless rise shows Labour pl...  \n",
      "12  [6 hrs ago Africa 'It's their loss': South Afr...  \n",
      "13  [Taliban order women to wear burkas to access ...  \n",
      "14  [How did it happen?, 2 days ago World Moment n...  \n",
      "15  [4 days ago Asia Dozens detained after rare pr...  \n",
      "16  [3 days ago 3 days ago Once a sure thing, Aust...  \n",
      "17  [Watch/Listen Watch: Russia's AI robot falls s...  \n",
      "News News Israel-Gaza war Israel receives body Hamas says belongs to hostage Confirmation of the remains would mean that only three deceased hostages' bodies now remain in Gaza. 3 hrs ago Middle East Israel has destroyed more than 1,500 buildings in Gaza since ceasefire Entire neighbourhoods controlled by Israel have been levelled in less than a month, the images show. 2 days ago Middle East Israel says body of Lior Rudaeff has been returned from Gaza Lior Rudaeff died defending Nir Yitzhak kibb\n",
      "{'avg_label': 'NEGATIVE', 'avg_score': 0.8281463936523155, 'num_sentences': 27, 'polarity_counts': {'positive': 2, 'negative': 25}, 'top_positive': [\"Bowen: Trump's role in Gaza ceasefire was decisive, but not a roadmap to peace Trump's Middle East visit was a victory lap - but peace does not emerge just because a president decides it.\", \"'I'm 89 and I saw my homeland rebuilt before - but now I don't believe Gaza has a future' One Palestinian family shares their story of life coming full circle - and what they believe lies ahead as talk turns to rebuilding Gaza Watch and listen again on iPlayer and Sounds Jeremy Bowen: Chances of peace in the Middle East The BBC's International Editor reflects on the ongoing peace efforts Latest updates 3 hrs ago Israel receives body Hamas says belongs to hostage Confirmation of the remains would mean that only three deceased hostages' bodies now remain in Gaza.\"], 'top_negative': [\"UN's top court says Israel obliged to allow UN aid into Gaza The court also says Israel has not substantiated its allegations against the UN agency for Palestinian refugees.\", \"Gaza doctors struggle to investigate 'signs of torture' on unnamed dead returned by Israel Photographs show bodies with multiple signs of injury and wrists tied behind their backs.\", \"Human stories Inside Gaza, BBC sees total devastation after two years of war With endless rubble and destroyed streets as far as the eye can see, any plans for Gaza's future are a far cry from where it is today, writes Lucy Williamson.\"], 'sentence_scores': [{'sentence': \"News News Israel-Gaza war Israel receives body Hamas says belongs to hostage Confirmation of the remains would mean that only three deceased hostages' bodies now remain in Gaza.\", 'label': 'NEGATIVE', 'score': 0.9957842230796814, 'value': -0.9957842230796814}, {'sentence': '3 hrs ago Middle East Israel has destroyed more than 1,500 buildings in Gaza since ceasefire Entire neighbourhoods controlled by Israel have been levelled in less than a month, the images show.', 'label': 'NEGATIVE', 'score': 0.9603792428970337, 'value': -0.9603792428970337}, {'sentence': '2 days ago Middle East Israel says body of Lior Rudaeff has been returned from Gaza Lior Rudaeff died defending Nir Yitzhak kibbutz during the Hamas attack on 7 October 2023, the Israeli military said.', 'label': 'NEGATIVE', 'score': 0.9940874576568604, 'value': -0.9940874576568604}, {'sentence': '5 days ago Middle East Released Israeli hostage says he was sexually assaulted in Gaza captivity In an interview with Israeli TV, Rom Braslavski described how he was stripped naked and tied up by members of Palestinian Islamic Jihad.', 'label': 'NEGATIVE', 'score': 0.978717029094696, 'value': -0.978717029094696}, {'sentence': '6 days ago Middle East Hamas fighters trapped in tunnels present new obstacle to Gaza ceasefire progress Mediators are trying to negotiate the exit of scores of fighters believed to hiding in a southern area of Gaza under Israeli control.', 'label': 'NEGATIVE', 'score': 0.9967543482780457, 'value': -0.9967543482780457}, {'sentence': '2 days ago World Israel passes first reading of bill proposing death penalty for people it deems terrorists The requirement means the penalty would likely be used only against Palestinians convicted of deadly attacks on Israelis.', 'label': 'NEGATIVE', 'score': 0.9818597435951233, 'value': -0.9818597435951233}, {'sentence': \"2 days ago Middle East Israel receives body of soldier killed in 2014 in Gaza The Israeli military confirmed the soldier's body had been returned from Gaza after 11 years on Sunday.\", 'label': 'NEGATIVE', 'score': 0.9575283527374268, 'value': -0.9575283527374268}, {'sentence': \"4 days ago Middle East Analysis The battle over Gaza's future: Why no-one can agree on the rebuild From AI-powered super cities to designs that capture its 'soul and spirit', plans have already been draw up - but political obstacles remain Can the Gaza ceasefire deal survive?\", 'label': 'NEGATIVE', 'score': 0.9878255724906921, 'value': -0.9878255724906921}, {'sentence': 'Its prospects depend heavily on the continuing engagement of the Trump White House, writes Frank Gardner.', 'label': 'NEGATIVE', 'score': 0.8607390522956848, 'value': -0.8607390522956848}, {'sentence': \"Watch: UN official says Gaza ruins 'like Hiroshima' Humanitarian chief Tom Fletcher talk to the BBC about scenes of despair he witnessed on a recent trip to the costal enclave.\", 'label': 'NEGATIVE', 'score': 0.9950723052024841, 'value': -0.9950723052024841}, {'sentence': 'New images show Israeli control line deeper into Gaza than expected Israel has placed boundary markers up to 520m deeper inside Gaza than expected under the ceasefire deal with Hamas.', 'label': 'NEGATIVE', 'score': 0.9181234836578369, 'value': -0.9181234836578369}, {'sentence': \"Bowen: Trump's role in Gaza ceasefire was decisive, but not a roadmap to peace Trump's Middle East visit was a victory lap - but peace does not emerge just because a president decides it.\", 'label': 'POSITIVE', 'score': 0.9894429445266724, 'value': 0.9894429445266724}, {'sentence': \"Human stories Inside Gaza, BBC sees total devastation after two years of war With endless rubble and destroyed streets as far as the eye can see, any plans for Gaza's future are a far cry from where it is today, writes Lucy Williamson.\", 'label': 'NEGATIVE', 'score': 0.9970365762710571, 'value': -0.9970365762710571}, {'sentence': 'Gaza children dying as they wait for Israel to enable evacuations Around 15,000 Gazans are waiting for urgent medical treatment, according to the UN.', 'label': 'NEGATIVE', 'score': 0.9963330030441284, 'value': -0.9963330030441284}, {'sentence': \"Gaza doctors struggle to investigate 'signs of torture' on unnamed dead returned by Israel Photographs show bodies with multiple signs of injury and wrists tied behind their backs.\", 'label': 'NEGATIVE', 'score': 0.9971133470535278, 'value': -0.9971133470535278}, {'sentence': \"UN's top court says Israel obliged to allow UN aid into Gaza The court also says Israel has not substantiated its allegations against the UN agency for Palestinian refugees.\", 'label': 'NEGATIVE', 'score': 0.9991759657859802, 'value': -0.9991759657859802}, {'sentence': \"'I'm 89 and I saw my homeland rebuilt before - but now I don't believe Gaza has a future' One Palestinian family shares their story of life coming full circle - and what they believe lies ahead as talk turns to rebuilding Gaza Watch and listen again on iPlayer and Sounds Jeremy Bowen: Chances of peace in the Middle East The BBC's International Editor reflects on the ongoing peace efforts Latest updates 3 hrs ago Israel receives body Hamas says belongs to hostage Confirmation of the remains would mean that only three deceased hostages' bodies now remain in Gaza.\", 'label': 'POSITIVE', 'score': 0.9643044471740723, 'value': 0.9643044471740723}, {'sentence': '3 hrs ago 1 day ago Gazan babies will die without aid - UN humanitarian chief About 14,000 babies will die in 48 hours if aid does not reach them, a UN humanitarian chief warns.', 'label': 'NEGATIVE', 'score': 0.9952989220619202, 'value': -0.9952989220619202}, {'sentence': '1 day ago 2 days ago Israel has destroyed more than 1,500 buildings in Gaza since ceasefire Entire neighbourhoods controlled by Israel have been levelled in less than a month, the images show.', 'label': 'NEGATIVE', 'score': 0.8787015676498413, 'value': -0.8787015676498413}, {'sentence': '2 days ago 2 days ago Hamas fighters trapped in tunnels present new obstacle to Gaza ceasefire progress Mediators are trying to negotiate the exit of scores of fighters believed to hiding in a southern area of Gaza under Israeli control.', 'label': 'NEGATIVE', 'score': 0.9963685274124146, 'value': -0.9963685274124146}, {'sentence': '2 days ago 2 days ago Israel passes first reading of bill proposing death penalty for people it deems terrorists The requirement means the penalty would likely be used only against Palestinians convicted of deadly attacks on Israelis.', 'label': 'NEGATIVE', 'score': 0.990626335144043, 'value': -0.990626335144043}, {'sentence': '2 days ago 3 days ago British journalist to be freed from US immigration detention Sami Hamdi, a British journalist and outspoken critic of Israel, has been in ICE custody for weeks.', 'label': 'NEGATIVE', 'score': 0.9733518362045288, 'value': -0.9733518362045288}, {'sentence': \"3 days ago 4 days ago Israel receives body of soldier killed in 2014 in Gaza The Israeli military confirmed the soldier's body had been returned from Gaza after 11 years on Sunday.\", 'label': 'NEGATIVE', 'score': 0.9622552394866943, 'value': -0.9622552394866943}, {'sentence': '4 days ago 5 days ago Israel says body of Lior Rudaeff has been returned from Gaza Lior Rudaeff died defending Nir Yitzhak kibbutz during the Hamas attack on 7 October 2023, the Israeli military said.', 'label': 'NEGATIVE', 'score': 0.9937078952789307, 'value': -0.9937078952789307}, {'sentence': '5 days ago 6 days ago Who are the released hostages?', 'label': 'NEGATIVE', 'score': 0.9958763122558594, 'value': -0.9958763122558594}, {'sentence': 'The Israeli military says 20 living hostages have been released by Hamas and have returned to Israel.', 'label': 'NEGATIVE', 'score': 0.9868934154510498, 'value': -0.9868934154510498}, {'sentence': '6 days ago 1 2 3 4 5 ... 12', 'label': 'NEGATIVE', 'score': 0.9240902662277222, 'value': -0.9240902662277222}]}\n"
     ]
    }
   ],
   "source": [
    "# Load a small fine-tuned sentiment model and bind to CPU (device=-1)\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "sent = pipeline(\"sentiment-analysis\", model=model_name, device=-1)\n",
    "\n",
    "def article_sentiment_analysis(text, max_sents=200, sent_trunc=590, batch_size=32):\n",
    "    \"\"\"\n",
    "    Per-article sentence-level sentiment:\n",
    "      - tokenizes into sentences\n",
    "      - runs batched inference via the pipeline\n",
    "      - converts labels/scores into signed values (positive = +score, negative = -score)\n",
    "      - returns averages, counts, top positive/negative sentences, and all sentence scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sents = sent_tokenize(text or \"\")                    # split text into sentences\n",
    "        if not sents:\n",
    "            return {                                         # empty-safe return\n",
    "                \"avg_label\": None, \"avg_score\": None,\n",
    "                \"num_sentences\": 0,\n",
    "                \"polarity_counts\": {\"positive\": 0, \"negative\": 0},\n",
    "                \"top_positive\": [], \"top_negative\": [], \"sentence_scores\": []\n",
    "            }\n",
    "\n",
    "        sents = sents[:max_sents]                           # cap number of sentences\n",
    "        sentence_scores = []\n",
    "\n",
    "        # Batch inference: truncate long sentences to avoid tokenizer/model limits\n",
    "        for i in range(0, len(sents), batch_size):\n",
    "            batch = [s[:sent_trunc] for s in sents[i:i+batch_size]]\n",
    "            results = sent(batch)                           # pipeline accepts list input\n",
    "            for txt, r in zip(batch, results):\n",
    "                label = r.get(\"label\")\n",
    "                score = float(r.get(\"score\", 0.0))\n",
    "                val = score if label == \"POSITIVE\" else -score\n",
    "                sentence_scores.append({\n",
    "                    \"sentence\": txt,\n",
    "                    \"label\": label,\n",
    "                    \"score\": score,\n",
    "                    \"value\": val\n",
    "                })\n",
    "\n",
    "        values = [s[\"value\"] for s in sentence_scores]\n",
    "        if not values:\n",
    "            return {                                         # no scored sentences\n",
    "                \"avg_label\": None, \"avg_score\": None,\n",
    "                \"num_sentences\": len(sents),\n",
    "                \"polarity_counts\": {\"positive\": 0, \"negative\": 0},\n",
    "                \"top_positive\": [], \"top_negative\": [], \"sentence_scores\": sentence_scores\n",
    "            }\n",
    "\n",
    "        # Aggregate per-article polarity and top sentences\n",
    "        avg_val = float(np.mean(values))\n",
    "        avg_label = \"POSITIVE\" if avg_val > 0 else \"NEGATIVE\"\n",
    "        avg_score = abs(avg_val)\n",
    "        pos_count = sum(1 for v in values if v > 0)\n",
    "        neg_count = sum(1 for v in values if v < 0)\n",
    "\n",
    "        sorted_by_val = sorted(sentence_scores, key=lambda x: x[\"value\"], reverse=True)\n",
    "        top_pos = [s[\"sentence\"] for s in sorted_by_val if s[\"value\"] > 0][:3]\n",
    "        top_neg = [s[\"sentence\"] for s in sorted_by_val if s[\"value\"] < 0][-3:][::-1]\n",
    "\n",
    "        return {\n",
    "            \"avg_label\": avg_label,\n",
    "            \"avg_score\": avg_score,\n",
    "            \"num_sentences\": len(sents),\n",
    "            \"polarity_counts\": {\"positive\": pos_count, \"negative\": neg_count},\n",
    "            \"top_positive\": top_pos,\n",
    "            \"top_negative\": top_neg,\n",
    "            \"sentence_scores\": sentence_scores\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Sentiment error:\", e)\n",
    "        return {                                             # graceful fallback on error\n",
    "            \"avg_label\": None, \"avg_score\": None,\n",
    "            \"num_sentences\": 0,\n",
    "            \"polarity_counts\": {\"positive\": 0, \"negative\": 0},\n",
    "            \"top_positive\": [], \"top_negative\": [], \"sentence_scores\": []\n",
    "        }\n",
    "\n",
    "# Apply function to each article's text, expand results to columns, and preview  \n",
    "results = df['text'].apply(article_sentiment_analysis)\n",
    "sent_df = pd.DataFrame(results.tolist())\n",
    "# Ensure run_date travels with the article-level output\n",
    "output = pd.concat([df[['title','url','text','run_date']].reset_index(drop=True), sent_df], axis=1)\n",
    "\n",
    "# Print key summary columns and example outputs for inspection\n",
    "print(output[['title','url','avg_label','avg_score','num_sentences','polarity_counts','top_positive','top_negative']])\n",
    "print(df.loc[0,'text'][:500])                     # peek at raw text of first article\n",
    "print(article_sentiment_analysis(df.loc[0,'text']))  # full sentiment dict for first article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efc8c314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 18 new rows to articles_sentiment_master.csv\n"
     ]
    }
   ],
   "source": [
    "# Append to master CSV \n",
    "master_path = \"articles_sentiment_master.csv\"\n",
    "master_cols = [\n",
    "    \"run_date\", \"title\", \"url\", \"avg_label\", \"avg_score\",\n",
    "    \"num_sentences\", \"polarity_counts\"\n",
    "]\n",
    "master_out = output[master_cols].copy()\n",
    "\n",
    "# De‑duplicate by run_date + url before writing\n",
    "if os.path.exists(master_path):\n",
    "    existing = pd.read_csv(master_path, usecols=[\"run_date\", \"url\"])\n",
    "    new_rows = master_out.merge(existing, on=[\"run_date\", \"url\"], how=\"left\", indicator=True)\n",
    "    master_out = new_rows[new_rows[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n",
    "\n",
    "# Append mode; write header only if file doesn't exist\n",
    "write_header = not os.path.exists(master_path)\n",
    "if not master_out.empty:\n",
    "    master_out.to_csv(master_path, mode=\"a\", header=write_header, index=False)\n",
    "    print(f\"Appended {len(master_out)} new rows to {master_path}\")\n",
    "else:\n",
    "    print(\"No new rows to append today (all duplicates).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "789eb918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: sentences_all_articles.csv, per-article CSVs in ./sentences/, and articles_sentiment_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Create output folders for sentence CSVs\n",
    "os.makedirs(\"sentences\", exist_ok=True)\n",
    "\n",
    "# Expand per-article sentence scores into a long table (one row per sentence)\n",
    "rows = []\n",
    "for idx, row in output.iterrows():\n",
    "    title = row.get('title', '')\n",
    "    url = row.get('url', '')\n",
    "    meta = {\n",
    "        \"article_index\": idx,\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"avg_label\": row.get('avg_label'),\n",
    "        \"avg_score\": row.get('avg_score'),\n",
    "        \"num_sentences\": row.get('num_sentences')\n",
    "    }\n",
    "    for s in row.get('sentence_scores', []):            # flatten sentence-level dicts\n",
    "        rows.append({\n",
    "            **meta,\n",
    "            \"sentence\": s.get('sentence'),\n",
    "            \"sent_label\": s.get('label'),\n",
    "            \"sent_score\": s.get('score'),\n",
    "            \"sent_value\": s.get('value')               # signed polarity value\n",
    "        })\n",
    "\n",
    "sent_long = pd.DataFrame(rows)                         # long-form dataframe for analysis\n",
    "\n",
    "# Save full sentence-level table for external use # Date-stamped filename to preserve previous runs\n",
    "sentences_all_path = f\"sentences_all_articles_{RUN_DATE}.csv\"\n",
    "sent_long.to_csv(sentences_all_path, index=False)\n",
    "# Also maintain the classic filename so your final print remains truthful\n",
    "sent_long.to_csv(\"sentences_all_articles.csv\", index=False)\n",
    "\n",
    "# For each article: save per-article CSV and write top sentences \n",
    "for idx, grp in sent_long.groupby(\"article_index\"):\n",
    "    title_safe = f\"article_{idx}\"\n",
    "    # Date-stamped names to avoid overwriting\n",
    "    article_dir_csv = os.path.join(\"sentences\", f\"{title_safe}_{RUN_DATE}.csv\")\n",
    "    grp.to_csv(article_dir_csv, index=False)        # per-article sentence CSV\n",
    "    grp.to_csv(os.path.join(\"sentences\", f\"{title_safe}.csv\"), index=False)\n",
    "\n",
    "    # Select top positive and most negative sentences for quick inspection\n",
    "    top_pos = grp[grp['sent_value'] > 0].sort_values('sent_value', ascending=False).head(3)\n",
    "    top_neg = grp[grp['sent_value'] < 0].sort_values('sent_value').head(3)\n",
    "\n",
    "    # Write a small text summary with top sentences\n",
    "    dated_top_path = os.path.join(\"sentences\", f\"{title_safe}_{RUN_DATE}_top_sentences.txt\")\n",
    "    with open(dated_top_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Title: {output.at[idx,'title']}\\nURL: {output.at[idx,'url']}\\n\\n\")\n",
    "        f.write(\"Top positive sentences:\\n\")\n",
    "        for i, r in top_pos.iterrows():\n",
    "            f.write(f\"- ({r['sent_score']:.3f}) {r['sentence']}\\n\")\n",
    "        f.write(\"\\nTop negative sentences:\\n\")\n",
    "        for i, r in top_neg.iterrows():\n",
    "            f.write(f\"- ({r['sent_score']:.3f}) {r['sentence']}\\n\")\n",
    "\n",
    "    # Also maintain the classic filename silently\n",
    "    classic_path = os.path.join(\"sentences\", f\"{title_safe}_top_sentences.txt\")\n",
    "    with open(classic_path, \"w\", encoding=\"utf-8\") as f2:\n",
    "        f2.write(f\"Title: {output.at[idx,'title']}\\nURL: {output.at[idx,'url']}\\n\\n\")\n",
    "        f2.write(\"Top positive sentences:\\n\")\n",
    "        for i, r in top_pos.iterrows():\n",
    "            f2.write(f\"- ({r['sent_score']:.3f}) {r['sentence']}\\n\")\n",
    "        f2.write(\"\\nTop negative sentences:\\n\")\n",
    "        for i, r in top_neg.iterrows():\n",
    "            f2.write(f\"- ({r['sent_score']:.3f}) {r['sentence']}\\n\")\n",
    "\n",
    "# Aggregate per-article polarity counts and mean signed score\n",
    "polarity_summary = sent_long.groupby('article_index')['sent_value'].apply(\n",
    "    lambda vals: pd.Series({\n",
    "        \"positive_count\": (vals > 0).sum(),\n",
    "        \"negative_count\": (vals < 0).sum(),\n",
    "        \"neutral_count\": (vals == 0).sum(),\n",
    "        \"mean_signed\": vals.mean()\n",
    "    })\n",
    ").unstack().reset_index()\n",
    "\n",
    "# Merge summary with article-level output and save final CSV\n",
    "summary_df = output.merge(polarity_summary, left_on=output.index, right_on='article_index', \n",
    "                        how='left').drop(columns=['key_0','article_index'], errors='ignore')\n",
    "summary_df.to_csv(f\"articles_sentiment_summary_{RUN_DATE}.csv\", index=False)\n",
    "# Also maintain classic filename so your print remains truthful\n",
    "summary_df.to_csv(\"articles_sentiment_summary.csv\", index=False)\n",
    "\n",
    "print(\"Saved: sentences_all_articles.csv, per-article CSVs in ./sentences/, and articles_sentiment_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5b0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save full article-level sentiment table to CSV for later analysis\n",
    "def save_outputs(output_df):\n",
    "    \"\"\"Save article-level outputs and derive sentence-level files/histograms.\"\"\"\n",
    "    output_df.to_csv(f\"articles_sentiment_{RUN_DATE}.csv\", index=False)\n",
    "    output_df.to_csv(\"articles_sentiment.csv\", index=False)\n",
    "\n",
    "# Write a human-readable text file with all articles (simple archive)\n",
    "with open(f\"articles_texts_{RUN_DATE}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, r in output.iterrows():\n",
    "        title = r.get('title', '').replace('\\n', ' ')   # keep single-line titles\n",
    "        url = r.get('url', '')\n",
    "        text = r.get('text', '').replace('\\r', '')      # normalize line endings\n",
    "        f.write(f\"## {i+1} {title}\\n{url}\\n{text}\\n\\n\")\n",
    "        \n",
    "with open(\"articles_texts.txt\", \"w\", encoding=\"utf-8\") as f2:\n",
    "    for i, r in output.iterrows():\n",
    "        title = r.get('title', '').replace('\\n', ' ')\n",
    "        url = r.get('url', '')\n",
    "        text = r.get('text', '').replace('\\r', '')\n",
    "        f2.write(f\"## {i+1} {title}\\n{url}\\n{text}\\n\\n\")\n",
    "\n",
    "# Cleanly close the Selenium browser and end the WebDriver session\n",
    "driver.quit()  # releases browser resources before the script exits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586af720",
   "metadata": {},
   "source": [
    "### ***File aggregation and histogram visualization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff3c3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV file paths for the last 7 days\n",
    "files = [\n",
    "    \"../News_Sentiment/sentences_all_articles_2025-11-07.csv\",\n",
    "    \"../News_Sentiment/sentences_all_articles_2025-11-08.csv\",\n",
    "    \"../News_Sentiment/sentences_all_articles_2025-11-09.csv\",\n",
    "    \"../News_Sentiment/sentences_all_articles_2025-11-10.csv\",\n",
    "    \"../News_Sentiment/sentences_all_articles_2025-11-11.csv\",\n",
    "    \"../News_Sentiment/sentences_all_articles_2025-11-12.csv\",\n",
    "    \"../News_Sentiment/sentences_all_articles_2025-11-13.csv\"     \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "768d4d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined_last_7d.csv\n",
      "Histogram saved as polarity_last_7d.png\n"
     ]
    }
   ],
   "source": [
    "# Function to generate histogram from 'sent_value'\n",
    "def save_polarity_histogram_from_list(csv_files, out_png=\"histogram.png\"):\n",
    "    try:\n",
    "        # Read each CSV into a DataFrame and collect them\n",
    "        df_list = [pd.read_csv(f) for f in csv_files]\n",
    "        # Combine all DataFrames into one for aggregate plotting\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        # Ensure required column exists before plotting\n",
    "        if 'sent_value' not in combined_df.columns:\n",
    "            raise ValueError(\"Column 'sent_value' not found in combined data.\")\n",
    "\n",
    "        # Create and save a histogram of sentiment values\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        combined_df['sent_value'].hist(bins=30, color='salmon', edgecolor='black')   \n",
    "        plt.title(\"Sentiment Polarity Histogram\")\n",
    "        plt.xlabel(\"Sentiment Value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_png)\n",
    "        plt.close()\n",
    "        print(f\"Histogram saved as {out_png}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to generate histogram:\", e)\n",
    "\n",
    "# Main block: read files, combine, assign index, save CSV, and plot\n",
    "try:\n",
    "    dfs = []\n",
    "    missing = []\n",
    "    for f in files:\n",
    "        # Check file exists before attempting read to avoid crashes\n",
    "        if os.path.exists(f):\n",
    "            dfs.append(pd.read_csv(f))\n",
    "        else:\n",
    "            missing.append(f)\n",
    "\n",
    "    # Report any paths that were not found\n",
    "    if missing:\n",
    "        print(\"Missing files (skipped):\", missing)\n",
    "    if not dfs:\n",
    "        raise FileNotFoundError(\"No input CSVs found.\")\n",
    "\n",
    "    # Concatenate all loaded DataFrames into a single table\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    # Add a monotonically increasing index for articles (useful as a stable identifier)\n",
    "    combined[\"article_index\"] = range(len(combined))\n",
    "    # Persist combined data for downstream use\n",
    "    combined.to_csv(\"combined_last_7d.csv\", index=False)\n",
    "    print(\"Saved combined_last_7d.csv\")\n",
    "\n",
    "    # # Generate histogram from the combined CSV file\n",
    "    save_polarity_histogram_from_list([\"combined_last_7d.csv\"], out_png=\"polarity_last_7d.png\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch-all to avoid unhandled exceptions in scripting contexts\n",
    "    print(\"Failed to build combined CSV or histogram:\", e)\n",
    "\n",
    "# Ensure all matplotlib figures are closed (release resources)    \n",
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
